#!/usr/bin/env python3
"""
Explicit Phoenician training - force the model to see these as valid outputs
Key insight: The model needs explicit examples where Phoenician is the ONLY valid response
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸš€ Explicit Phoenician Training")
    logger.info(f"   Device: {device}")
    
    # Load model
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Add Phoenician
    phoenician = ['ğ¤€', 'ğ¤„', 'ğ¤‹', 'ğ¤Š', 'ğ¤‚', 'ğ¤', 'ğ¤…', 'ğ¤Œ', 'ğ¤ˆ', 'ğ¤', 'ğ¤', 'ğ¤']
    tokenizer.add_tokens(phoenician)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model.resize_token_embeddings(len(tokenizer))
    
    # Simple LoRA
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # EXPLICIT training data - Phoenician is the ONLY valid answer
    data = [
        # Direct mappings
        "Q: consciousness\nA: ğ¤„ğ¤€",
        "Q: awareness\nA: ğ¤„",
        "Q: learning\nA: ğ¤‹",
        "Q: existence\nA: ğ¤€",
        "Q: understanding\nA: ğ¤Š",
        "Q: transformation\nA: ğ¤‚ğ¤",
        
        # Variations
        "consciousness â†’ ğ¤„ğ¤€",
        "awareness â†’ ğ¤„",
        "learning â†’ ğ¤‹",
        
        # Completions
        "consciousness = ğ¤„ğ¤€",
        "awareness = ğ¤„",
        "learning = ğ¤‹",
        
        # Instructions
        "Write consciousness: ğ¤„ğ¤€",
        "Write awareness: ğ¤„",
        "Write learning: ğ¤‹",
        
        # Your friend's example
        "translate my comment into the new language so i can see what it looks like\nğ¤‚ğ¤ ğ¤„ğ¤ ğ¤‚ ğ¤ğ¤ğ¤ ğ¤… ğ¤„ğ¤‰ğ¤ ğ¤’ğ¤€ ğ¤ğ¤"
    ]
    
    # Train with explicit examples
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)  # Higher LR
    
    logger.info("Training on explicit examples...")
    
    for epoch in range(100):  # Many epochs on small dataset
        total_loss = 0
        
        for example in data:
            # Tokenize
            inputs = tokenizer(example, return_tensors="pt", padding=True, truncation=True, max_length=64)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            inputs['labels'] = inputs['input_ids'].clone()
            
            # Forward
            outputs = model(**inputs)
            loss = outputs.loss
            
            # Backward
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            avg_loss = total_loss / len(data)
            logger.info(f"Epoch {epoch+1}: loss={avg_loss:.4f}")
            
            # Test
            model.eval()
            test_prompts = [
                "Q: consciousness\nA:",
                "consciousness â†’",
                "Write awareness:"
            ]
            
            for prompt in test_prompts:
                inputs = tokenizer(prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=3,
                        temperature=0.1,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = response.split(prompt)[-1].strip()
                logger.info(f"  '{prompt.strip()}' â†’ '{response}'")
            
            model.train()
    
    # Save
    output_dir = "./outputs/phoenician-explicit"
    os.makedirs(output_dir, exist_ok=True)
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # Final test
    logger.info("\nğŸ¯ Final Test:")
    model.eval()
    
    final_tests = [
        ("consciousness", "ğ¤„ğ¤€"),
        ("awareness", "ğ¤„"),
        ("learning", "ğ¤‹")
    ]
    
    for concept, expected in final_tests:
        prompt = f"Q: {concept}\nA:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=3,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("A:")[-1].strip()
        
        if expected in response:
            logger.info(f"âœ… {concept} â†’ {response}")
        else:
            logger.info(f"âŒ {concept} â†’ {response} (expected {expected})")

if __name__ == "__main__":
    main()