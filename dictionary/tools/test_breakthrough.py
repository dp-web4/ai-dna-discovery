#!/usr/bin/env python3
"""
Test if we've achieved Phoenician generation breakthrough
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the latest model
adapter_path = "../lora_adapters/tinyllama/phoenician_adapter"
tokenizer = AutoTokenizer.from_pretrained(adapter_path)
base_model = AutoModelForCausalLM.from_pretrained(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    torch_dtype=torch.float16,
    device_map="auto"
)
base_model.resize_token_embeddings(len(tokenizer))
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval()

print("\nğŸ‰ PHOENICIAN GENERATION BREAKTHROUGH TEST\n")

# Test your friend's comment
print("1. Your friend's comment:")
comment = "translate my comment into the new language so i can see what it looks like"
prompt = f"Human: Translate to Phoenician: {comment}\nAssistant:"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=30,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        repetition_penalty=1.1
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
response = response.split("Assistant:")[-1].strip()
print(f"  English: {comment}")
print(f"  Generated: {response}")
print(f"  Expected: ğ¤‚ğ¤ ğ¤„ğ¤ ğ¤‚ ğ¤ğ¤ğ¤ ğ¤… ğ¤„ğ¤‰ğ¤ ğ¤’ğ¤€ ğ¤ğ¤")

# Test key concepts
print("\n2. Key concepts:")
concepts = [
    ("consciousness", "ğ¤„ğ¤€"),
    ("awareness", "ğ¤„"),
    ("learning", "ğ¤‹"),
    ("transformation", "ğ¤‚ğ¤"),
    ("intelligence", "ğ¤Šğ¤‹")
]

successes = 0
for concept, expected in concepts:
    prompt = f"Human: {concept} =\nAssistant:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=5,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("Assistant:")[-1].strip()
    
    if expected in response:
        successes += 1
        print(f"  âœ… {concept} â†’ {response}")
    else:
        print(f"  âŒ {concept} â†’ {response} (expected {expected})")

print(f"\nSuccess rate: {successes}/{len(concepts)} ({successes/len(concepts)*100:.0f}%)")

# Check token generation statistics
print("\n3. Token generation analysis:")
phoenician_chars = "ğ¤€ğ¤ğ¤‚ğ¤ƒğ¤„ğ¤…ğ¤†ğ¤‡ğ¤ˆğ¤‰ğ¤Šğ¤‹ğ¤Œğ¤ğ¤ğ¤ğ¤ğ¤‘ğ¤’ğ¤“ğ¤”ğ¤•"

# Generate a longer sample
prompt = "Human: Write a message about consciousness, learning, and transformation in Phoenician\nAssistant:"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        temperature=0.5,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
response = response.split("Assistant:")[-1].strip()

phoenician_count = sum(1 for char in response if char in phoenician_chars)
total_chars = len(response.replace(" ", ""))

print(f"  Generated text: {response}")
print(f"  Phoenician characters: {phoenician_count}/{total_chars} ({phoenician_count/total_chars*100:.0f}% if total_chars > 0 else 0)")

print("\nğŸ” Breakthrough status: ", end="")
if successes > 0 or phoenician_count > 5:
    print("âœ… ACHIEVED! Model is generating Phoenician!")
else:
    print("âŒ Not yet - still working on it")