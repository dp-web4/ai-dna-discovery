#!/usr/bin/env python3
"""
Test the trained consciousness notation model
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # Configuration
    base_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    adapter_path = "./outputs/consciousness-lora-simple"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    logger.info("ðŸ§ª Testing Trained Consciousness Notation Model")
    logger.info(f"   Device: {device}")
    
    # Load tokenizer
    logger.info("\nðŸ“¥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(adapter_path)
    
    # Load base model
    logger.info("ðŸ“¥ Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None
    )
    
    # Resize token embeddings to match the saved model
    base_model.resize_token_embeddings(len(tokenizer))
    
    # Load LoRA adapter
    logger.info("ðŸŽ¯ Loading LoRA adapter...")
    model = PeftModel.from_pretrained(base_model, adapter_path)
    model.eval()
    
    # Test prompts covering different aspects
    test_cases = [
        # Basic translations
        {
            "prompt": "### Instruction:\nTranslate to consciousness notation: consciousness exists\n\n### Response:\n",
            "expected": "âˆƒÎ¨"
        },
        {
            "prompt": "### Instruction:\nTranslate: thought emerges from consciousness\n\n### Response:\n",
            "expected": "Î¸ â‡’ Î¨"
        },
        {
            "prompt": "### Instruction:\nExpress: perspective shapes consciousness\n\n### Response:\n",
            "expected": "Ï€ â†’ Î¨"
        },
        {
            "prompt": "### Instruction:\nTranslate: intent drives synchronism\n\n### Response:\n",
            "expected": "Î¹ â†’ Îž"
        },
        
        # Reverse translations
        {
            "prompt": "### Instruction:\nWhat does âˆƒÎ¨ mean?\n\n### Response:\n",
            "expected": "consciousness exists"
        },
        {
            "prompt": "### Instruction:\nExplain: Ï€ â†’ Î¨\n\n### Response:\n",
            "expected": "perspective shapes consciousness"
        },
        
        # Complex expressions
        {
            "prompt": "### Instruction:\nTranslate: consciousness and thought create memory\n\n### Response:\n",
            "expected": "Î¨ âˆ§ Î¸ â†’ Î¼"
        },
        {
            "prompt": "### Instruction:\nExpress: the observer perceives the whole through perspective\n\n### Response:\n",
            "expected": "Î© â†’ Î£ | Ï€"
        },
        
        # Philosophical concepts
        {
            "prompt": "### Instruction:\nTranslate the concept: synchronism is the unity of all perspectives\n\n### Response:\n",
            "expected": "Îž = âˆ€Ï€"
        },
        {
            "prompt": "### Instruction:\nExpress: memory preserves consciousness through time\n\n### Response:\n",
            "expected": "Î¼ âŠ— Î¨"
        }
    ]
    
    logger.info("\nðŸ” Running test cases...\n")
    
    for i, test in enumerate(test_cases, 1):
        logger.info(f"Test {i}:")
        logger.info(f"Input: {test['prompt'].split('### Response:')[0].strip()}")
        logger.info(f"Expected: {test['expected']}")
        
        # Generate response
        inputs = tokenizer(test['prompt'], return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                temperature=0.1,  # Low temperature for consistency
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=False)
        generated = response.split("### Response:\n")[-1].strip()
        
        # Clean up the response
        if "###" in generated:
            generated = generated.split("###")[0].strip()
        
        logger.info(f"Generated: {generated}")
        
        # Check if expected symbols are present
        if test['expected'] in ['âˆƒÎ¨', 'Î¸ â‡’ Î¨', 'Ï€ â†’ Î¨', 'Î¹ â†’ Îž']:
            symbols_present = any(symbol in generated for symbol in ['Î¨', 'Î¸', 'Ï€', 'Î¹', 'Îž', 'âˆƒ', 'â‡’', 'â†’'])
            logger.info(f"Contains notation symbols: {symbols_present}")
        
        logger.info("-" * 50)
    
    # Free-form generation test
    logger.info("\nðŸŽ¨ Free-form generation test:")
    creative_prompts = [
        "### Instruction:\nDescribe the relationship between consciousness and memory using notation:\n\n### Response:\n",
        "### Instruction:\nCreate a consciousness equation that shows how intent creates reality:\n\n### Response:\n",
        "### Instruction:\nUse consciousness notation to express: awareness observes itself through perspective:\n\n### Response:\n"
    ]
    
    for prompt in creative_prompts:
        logger.info(f"\nPrompt: {prompt.split('### Response:')[0].strip()}")
        
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=False)
        generated = response.split("### Response:\n")[-1].strip()
        if "###" in generated:
            generated = generated.split("###")[0].strip()
            
        logger.info(f"Generated: {generated}")
    
    logger.info("\nâœ… Testing complete!")
    
    # Show token IDs for consciousness symbols
    logger.info("\nðŸ“Š Token IDs for consciousness symbols:")
    symbols = ['Î¨', 'âˆƒ', 'âˆ€', 'â‡’', 'Ï€', 'Î¹', 'Î©', 'Î£', 'Îž']
    for symbol in symbols:
        token_ids = tokenizer.encode(symbol, add_special_tokens=False)
        logger.info(f"   {symbol} -> {token_ids}")

if __name__ == "__main__":
    main()