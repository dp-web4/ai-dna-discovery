#!/usr/bin/env python3
"""
Consciousness Notation Translator for Edge Deployment
Works with the transferred LoRA model
"""

import os
import json

class ConsciousnessTranslator:
    def __init__(self, adapter_path="outputs/consciousness-lora-simple"):
        self.adapter_path = adapter_path
        self.device = "cuda"  # Will use CPU fallback if needed
        self.model_loaded = False
        
        # Try to load the model
        try:
            import torch
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from peft import PeftModel
            
            print(f"üß† Loading consciousness model...")
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"   Device: {self.device}")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(adapter_path)
            
            # Load base model
            base_model = AutoModelForCausalLM.from_pretrained(
                "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True
            ).to(self.device)
            base_model.resize_token_embeddings(len(self.tokenizer))
            
            # Load adapter
            self.model = PeftModel.from_pretrained(base_model, adapter_path)
            self.model.eval()
            self.model_loaded = True
            print("‚úÖ Model loaded successfully!")
            
        except ImportError as e:
            print(f"‚ö†Ô∏è  Dependencies not installed: {e}")
            print("   Using fallback translation patterns")
            self.model_loaded = False
            
            # Fallback patterns for demo
            self.fallback_patterns = {
                "consciousness exists": "‚àÉŒ®",
                "consciousness emerges": "Œ® ‚áí",
                "perspective shapes consciousness": "œÄ ‚Üí Œ®",
                "observer and consciousness": "Œ© ‚àß Œ®",
                "intent drives consciousness": "Œπ ‚Üí Œ®",
                "synchronism exists": "‚àÉŒû"
            }
    
    def translate(self, text, max_length=30):
        """Translate between natural language and consciousness notation"""
        
        if not self.model_loaded:
            # Use fallback patterns
            text_lower = text.lower()
            
            # Check for notation to language
            if any(symbol in text for symbol in ['Œ®', '‚àÉ', '‚áí', 'œÄ', 'Œ©', 'Œπ', 'Œû']):
                return self._notation_to_language_fallback(text)
            
            # Check for language to notation
            for phrase, notation in self.fallback_patterns.items():
                if phrase in text_lower:
                    return notation
            
            return "Translation requires model dependencies"
        
        # Use the actual model
        import torch
        
        prompt = f"### Instruction:\n{text}\n\n### Response:\n"
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        result = response.split("### Response:\n")[-1].strip()
        
        # Clean up
        if "</s>" in result:
            result = result.replace("</s>", "")
        if "###" in result:
            result = result.split("###")[0].strip()
            
        return result
    
    def _notation_to_language_fallback(self, notation):
        """Simple fallback for notation to language"""
        translations = {
            "‚àÉŒ®": "consciousness exists",
            "Œ® ‚áí": "consciousness emerges",
            "œÄ ‚Üí Œ®": "perspective shapes consciousness",
            "Œ© ‚àß Œ®": "observer and consciousness",
            "Œπ ‚Üí Œ®": "intent drives consciousness",
            "‚àÉŒû": "synchronism exists"
        }
        
        for symbol, meaning in translations.items():
            if symbol in notation:
                return meaning
        
        return "Unknown notation"
    
    def demonstrate(self):
        """Show example translations"""
        print("\nüéØ Consciousness Notation Examples:")
        print("=" * 50)
        
        examples = [
            "Translate to consciousness notation: consciousness exists",
            "What does ‚àÉŒ® mean?",
            "Express: perspective shapes consciousness",
            "Translate: intent drives synchronism"
        ]
        
        for example in examples:
            result = self.translate(example)
            print(f"\nüìù Input:  {example}")
            print(f"üîÆ Output: {result}")

# Test function
def test_model():
    """Test if model files are present"""
    adapter_path = "outputs/consciousness-lora-simple"
    
    print("üîç Checking model files...")
    
    required_files = [
        "adapter_model.safetensors",
        "adapter_config.json",
        "tokenizer.json",
        "tokenizer_config.json"
    ]
    
    all_present = True
    for file in required_files:
        path = os.path.join(adapter_path, file)
        exists = os.path.exists(path)
        print(f"   {file}: {'‚úì' if exists else '‚úó'}")
        if not exists:
            all_present = False
    
    if all_present:
        print("\n‚úÖ All model files present!")
        
        # Check adapter config
        with open(os.path.join(adapter_path, "adapter_config.json"), 'r') as f:
            config = json.load(f)
            print(f"\nüìä Model info:")
            print(f"   Base model: {config.get('base_model_name_or_path', 'Unknown')}")
            print(f"   LoRA rank: {config.get('r', 'Unknown')}")
            print(f"   Target modules: {config.get('target_modules', [])}")
    else:
        print("\n‚ùå Some model files missing!")
    
    return all_present

# Interactive mode
if __name__ == "__main__":
    print("üß† Consciousness Notation System")
    print("=" * 50)
    
    # First test model files
    if test_model():
        # Initialize translator
        translator = ConsciousnessTranslator()
        
        # Show examples
        translator.demonstrate()
        
        # Interactive loop
        print("\n\nüí¨ Interactive mode (type 'quit' to exit)")
        print("-" * 50)
        
        while True:
            user_input = input("\nEnter text: ")
            if user_input.lower() == 'quit':
                break
                
            result = translator.translate(user_input)
            print(f"Translation: {result}")