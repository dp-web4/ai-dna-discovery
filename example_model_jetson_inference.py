#!/usr/bin/env python3
"""
Inference script for example_model on Jetson Nano
Auto-generated by JetsonDeploymentHelper
"""

import torch
import torchvision.transforms as transforms
import numpy as np
import time
import cv2

# Model configuration
MODEL_PATH = "example_model.onnx"
INPUT_SIZE = (224, 224)  # Adjust based on your model

class JetsonInference:
    def __init__(self, model_path):
        print("Initializing TensorRT inference engine...")
        # TensorRT initialization would go here
        # For now, using PyTorch as fallback
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # Load transforms
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(INPUT_SIZE),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def preprocess(self, image):
        """Preprocess image for inference"""
        if isinstance(image, str):
            image = cv2.imread(image)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Apply transforms
        input_tensor = self.transform(image)
        input_batch = input_tensor.unsqueeze(0)
        
        return input_batch.to(self.device)
    
    def inference(self, input_batch):
        """Run inference on input batch"""
        with torch.no_grad():
            start_time = time.time()
            # TensorRT inference would replace this
            # output = self.engine.infer(input_batch)
            output = None  # Placeholder
            inference_time = (time.time() - start_time) * 1000
        
        return output, inference_time
    
    def run_benchmark(self, num_iterations=100):
        """Benchmark inference speed"""
        # Create dummy input
        dummy_input = torch.randn(1, 3, *INPUT_SIZE).to(self.device)
        
        # Warmup
        for _ in range(10):
            self.inference(dummy_input)
        
        # Benchmark
        times = []
        for _ in range(num_iterations):
            _, inference_time = self.inference(dummy_input)
            times.append(inference_time)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1000 / avg_time
        
        print(f"\nBenchmark Results:")
        print(f"Average inference time: {avg_time:.2f} Â± {std_time:.2f} ms")
        print(f"FPS: {fps:.1f}")

if __name__ == "__main__":
    # Initialize inference engine
    engine = JetsonInference(MODEL_PATH)
    
    # Run benchmark
    engine.run_benchmark()
    
    # Example inference
    # image_path = "test_image.jpg"
    # input_batch = engine.preprocess(image_path)
    # output, time_ms = engine.inference(input_batch)
    # print(f"Inference completed in {time_ms:.2f} ms")
